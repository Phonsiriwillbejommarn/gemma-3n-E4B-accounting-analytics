{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "caea00d3-1fbe-4246-aff1-35c45138277c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.12.11 environment at: /home/zeus/miniconda3/envs/cloudspace\u001b[0m\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m74 packages\u001b[0m \u001b[2min 366ms\u001b[0m\u001b[0m                                        \u001b[0m\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m55 packages\u001b[0m \u001b[2min 33.16s\u001b[0m\u001b[0m                                           \n",
      "\u001b[2K‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë [0/55] \u001b[2mInstalling wheels...                                \u001b[0m\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mFailed to hardlink files; falling back to full copy. This may lead to degraded performance.\n",
      "         If the cache and target directories are on different filesystems, hardlinking may not be supported.\n",
      "         If this is intentional, set `export UV_LINK_MODE=copy` or use `--link-mode=copy` to suppress this warning.\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m55 packages\u001b[0m \u001b[2min 5.46s\u001b[0m\u001b[0m                              \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1maccelerate\u001b[0m\u001b[2m==1.12.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1maiohappyeyeballs\u001b[0m\u001b[2m==2.6.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1maiohttp\u001b[0m\u001b[2m==3.13.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1maiosignal\u001b[0m\u001b[2m==1.4.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mbitsandbytes\u001b[0m\u001b[2m==0.49.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mclick\u001b[0m\u001b[2m==8.3.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcuda-bindings\u001b[0m\u001b[2m==12.9.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcuda-pathfinder\u001b[0m\u001b[2m==1.3.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdatasets\u001b[0m\u001b[2m==4.5.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdill\u001b[0m\u001b[2m==0.4.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfilelock\u001b[0m\u001b[2m==3.20.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfrozenlist\u001b[0m\u001b[2m==1.8.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfsspec\u001b[0m\u001b[2m==2025.10.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhf-xet\u001b[0m\u001b[2m==1.2.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhuggingface-hub\u001b[0m\u001b[2m==1.3.5\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmpmath\u001b[0m\u001b[2m==1.3.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmultidict\u001b[0m\u001b[2m==6.7.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmultiprocess\u001b[0m\u001b[2m==0.70.18\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnetworkx\u001b[0m\u001b[2m==3.6.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==2.4.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cublas-cu12\u001b[0m\u001b[2m==12.8.4.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-cupti-cu12\u001b[0m\u001b[2m==12.8.90\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-nvrtc-cu12\u001b[0m\u001b[2m==12.8.93\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-runtime-cu12\u001b[0m\u001b[2m==12.8.90\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cudnn-cu12\u001b[0m\u001b[2m==9.10.2.21\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cufft-cu12\u001b[0m\u001b[2m==11.3.3.83\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cufile-cu12\u001b[0m\u001b[2m==1.13.1.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-curand-cu12\u001b[0m\u001b[2m==10.3.9.90\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cusolver-cu12\u001b[0m\u001b[2m==11.7.3.90\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cusparse-cu12\u001b[0m\u001b[2m==12.5.8.93\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cusparselt-cu12\u001b[0m\u001b[2m==0.7.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-nccl-cu12\u001b[0m\u001b[2m==2.27.5\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-nvjitlink-cu12\u001b[0m\u001b[2m==12.8.93\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-nvshmem-cu12\u001b[0m\u001b[2m==3.4.5\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-nvtx-cu12\u001b[0m\u001b[2m==12.8.90\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpandas\u001b[0m\u001b[2m==3.0.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpeft\u001b[0m\u001b[2m==0.18.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpillow\u001b[0m\u001b[2m==12.1.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpropcache\u001b[0m\u001b[2m==0.4.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpyarrow\u001b[0m\u001b[2m==23.0.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mregex\u001b[0m\u001b[2m==2026.1.15\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msafetensors\u001b[0m\u001b[2m==0.7.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mshellingham\u001b[0m\u001b[2m==1.5.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msympy\u001b[0m\u001b[2m==1.14.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtimm\u001b[0m\u001b[2m==1.0.24\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtokenizers\u001b[0m\u001b[2m==0.22.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtorch\u001b[0m\u001b[2m==2.10.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtorchvision\u001b[0m\u001b[2m==0.25.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtqdm\u001b[0m\u001b[2m==4.67.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtransformers\u001b[0m\u001b[2m==5.0.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtriton\u001b[0m\u001b[2m==3.6.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtrl\u001b[0m\u001b[2m==0.27.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtyper-slim\u001b[0m\u001b[2m==0.21.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mxxhash\u001b[0m\u001b[2m==3.6.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1myarl\u001b[0m\u001b[2m==1.22.0\u001b[0m\n",
      "\u001b[2mUsing Python 3.12.11 environment at: /home/zeus/miniconda3/envs/cloudspace\u001b[0m\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m31 packages\u001b[0m \u001b[2min 179ms\u001b[0m\u001b[0m                                        \u001b[0m\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m10 packages\u001b[0m \u001b[2min 396ms\u001b[0m\u001b[0m                                            \n",
      "\u001b[2K‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë [0/10] \u001b[2mInstalling wheels...                                \u001b[0m\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mFailed to hardlink files; falling back to full copy. This may lead to degraded performance.\n",
      "         If the cache and target directories are on different filesystems, hardlinking may not be supported.\n",
      "         If this is intentional, set `export UV_LINK_MODE=copy` or use `--link-mode=copy` to suppress this warning.\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m10 packages\u001b[0m \u001b[2min 78ms\u001b[0m\u001b[0m                               \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mannotated-types\u001b[0m\u001b[2m==0.7.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mgitdb\u001b[0m\u001b[2m==4.0.12\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mgitpython\u001b[0m\u001b[2m==3.1.46\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mprotobuf\u001b[0m\u001b[2m==6.33.5\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpydantic\u001b[0m\u001b[2m==2.12.5\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpydantic-core\u001b[0m\u001b[2m==2.41.5\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msentry-sdk\u001b[0m\u001b[2m==2.51.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msmmap\u001b[0m\u001b[2m==5.0.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtyping-inspection\u001b[0m\u001b[2m==0.4.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mwandb\u001b[0m\u001b[2m==0.24.1\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# ‡∏£‡∏±‡∏ô‡πÉ‡∏ô Jupyter Cell\n",
    "!uv pip install torch transformers datasets accelerate peft trl bitsandbytes timm\n",
    "!uv pip install wandb huggingface_hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec50cdf0-d0b4-48f4-9dc3-f028804f7e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: [wandb.login()] Using explicit session credentials for https://api.wandb.ai.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /teamspace/studios/this_studio/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mthabunsri42\u001b[0m (\u001b[33mthabunsri42-suranaree-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W&B logged in successfully!\n",
      "Hugging Face logged in successfully!\n"
     ]
    }
   ],
   "source": [
    "MY_WANDB_KEY = \"api\"\n",
    "MY_HF_TOKEN = \"api\"\n",
    "\n",
    "\n",
    "try:\n",
    "    if MY_WANDB_KEY:\n",
    "        import wandb\n",
    "        wandb.login(key=MY_WANDB_KEY)\n",
    "        print(\"W&B logged in successfully!\")\n",
    "    else:\n",
    "        print(\"W&B Key is empty. Skipping W&B login.\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to login to W&B: {e}\")\n",
    "\n",
    "# --- 3. Login Hugging Face ---\n",
    "try:\n",
    "    if MY_HF_TOKEN:\n",
    "        from huggingface_hub import login\n",
    "        login(token=MY_HF_TOKEN)\n",
    "        print(\"Hugging Face logged in successfully!\")\n",
    "    else:\n",
    "        print(\"HF Token is empty. Skipping HF login.\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to login to Hugging Face: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4422108-8860-41f9-963a-62c40e39e77b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Repairing dataset: thai_autoparts_ai_generated (12).json...\n",
      "‚ö†Ô∏è Skipped broken line 7907\n",
      "‚úÖ Dataset repaired! Saved 7906 lines to 'cleaned_dataset.jsonl'\n",
      "Loading Tokenizer & Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9880af59189442c189f0311ac621cbbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/1676 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1d00193b42248db8803c83a715cc986",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7906 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8094db7fdf0349b5997bc4fc3bddded0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to train dataset:   0%|          | 0/7906 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c87dcbae70a74aae9825d9d085ccae18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/7906 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b7dad87913f44da8d29a8bd883b6858",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/7906 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting training on H100 (SDPA Mode)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: [wandb.login()] Loaded credentials for https://api.wandb.ai from /teamspace/studios/this_studio/.netrc.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mthabunsri42\u001b[0m (\u001b[33mthabunsri42-suranaree-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.24.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/teamspace/studios/this_studio/wandb/run-20260202_092208-5xfwaoas</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thabunsri42-suranaree-university/huggingface/runs/5xfwaoas' target=\"_blank\">gemma-3n-E4B-accounting-analytics-H100-run</a></strong> to <a href='https://wandb.ai/thabunsri42-suranaree-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thabunsri42-suranaree-university/huggingface' target=\"_blank\">https://wandb.ai/thabunsri42-suranaree-university/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thabunsri42-suranaree-university/huggingface/runs/5xfwaoas' target=\"_blank\">https://wandb.ai/thabunsri42-suranaree-university/huggingface/runs/5xfwaoas</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='122' max='248' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [122/248 07:00 < 07:21, 0.29 it/s, Epoch 0.49/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>15.473465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>6.732574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>5.374412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>4.479695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>4.031647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.447791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.869850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.719231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.601967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.813456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>2.360870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>2.312362</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments\n",
    ")\n",
    "from peft import LoraConfig\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from huggingface_hub import login\n",
    "\n",
    "# --- 0. Login & Variables ---\n",
    "MY_HF_TOKEN = \"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\"\n",
    "login(token=MY_HF_TOKEN)\n",
    "\n",
    "MODEL_ID = \"google/gemma-3n-E4B-it\"\n",
    "# ‡πÅ‡∏Å‡πâ‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡πâ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö‡πÑ‡∏ü‡∏•‡πå‡∏à‡∏£‡∏¥‡∏á‡πÉ‡∏ô‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á\n",
    "RAW_DATASET_PATH = \"thai_autoparts_ai_generated (12).json\" \n",
    "CLEAN_DATASET_PATH = \"cleaned_dataset.jsonl\"\n",
    "\n",
    "NEW_MODEL_NAME = \"gemma-3n-E4B-accounting-analytics\"\n",
    "YOUR_HF_USERNAME = \"Phonsiri\"\n",
    "HUB_REPO_ID = f\"{YOUR_HF_USERNAME}/{NEW_MODEL_NAME}\"\n",
    "OUTPUT_DIR = \"./results\"\n",
    "\n",
    "# --- 1. Fix Broken JSON Lines (Auto-Repair) ---\n",
    "print(f\"üîß Repairing dataset: {RAW_DATASET_PATH}...\")\n",
    "valid_lines = 0\n",
    "with open(RAW_DATASET_PATH, 'r', encoding='utf-8') as f_in, \\\n",
    "     open(CLEAN_DATASET_PATH, 'w', encoding='utf-8') as f_out:\n",
    "    for i, line in enumerate(f_in):\n",
    "        line = line.strip()\n",
    "        if not line: continue\n",
    "        try:\n",
    "            # ‡∏•‡∏≠‡∏á Parse JSON ‡∏ñ‡πâ‡∏≤‡∏ú‡πà‡∏≤‡∏ô‡∏Ñ‡πà‡∏≠‡∏¢‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå‡πÉ‡∏´‡∏°‡πà\n",
    "            json.loads(line) \n",
    "            f_out.write(line + '\\n')\n",
    "            valid_lines += 1\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"‚ö†Ô∏è Skipped broken line {i+1}\")\n",
    "            continue\n",
    "\n",
    "print(f\"‚úÖ Dataset repaired! Saved {valid_lines} lines to '{CLEAN_DATASET_PATH}'\")\n",
    "\n",
    "# --- 2. Load Tokenizer & Model (H100 + SDPA) ---\n",
    "print(\"Loading Tokenizer & Model...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"sdpa\",   # ‡πÉ‡∏ä‡πâ SDPA ‡∏ï‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏Ç‡∏≠\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# üî• [NEW] FIX TOKENIZER & MODEL CONFIG MISMATCH üî•\n",
    "# ‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ Warning: \"The tokenizer has new PAD/BOS/EOS tokens...\"\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÉ‡∏´‡πâ Model Config ‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ö Tokenizer\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.bos_token_id = tokenizer.bos_token_id\n",
    "model.config.eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# ‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï Generation Config ‡∏î‡πâ‡∏ß‡∏¢ (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)\n",
    "if hasattr(model, \"generation_config\"):\n",
    "    model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "    model.generation_config.bos_token_id = tokenizer.bos_token_id\n",
    "    model.generation_config.eos_token_id = tokenizer.eos_token_id\n",
    "    \n",
    "print(f\"‚úÖ Fixed Token IDs: PAD={tokenizer.pad_token_id}, BOS={tokenizer.bos_token_id}, EOS={tokenizer.eos_token_id}\")\n",
    "\n",
    "\n",
    "# --- 3. Prepare Dataset ---\n",
    "print(\"Preparing Dataset...\")\n",
    "# ‡πÉ‡∏ä‡πâ‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏ã‡πà‡∏≠‡∏°‡πÅ‡∏•‡πâ‡∏ß (CLEAN_DATASET_PATH)\n",
    "dataset = load_dataset(\"json\", data_files=CLEAN_DATASET_PATH, split=\"train\")\n",
    "\n",
    "def format_chat_template(row):\n",
    "    formatted_text = \"\"\n",
    "    # ‡πÉ‡∏ä‡πâ .get() ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á 'conversation' ‡πÅ‡∏•‡∏∞ 'messages' ‡∏Å‡∏±‡∏ô‡∏û‡∏•‡∏≤‡∏î\n",
    "    msgs = row.get(\"conversation\") or row.get(\"messages\")\n",
    "    \n",
    "    for message in msgs:\n",
    "        role = message[\"role\"]\n",
    "        content = message[\"content\"]\n",
    "        if role == \"user\":\n",
    "            formatted_text += f\"<start_of_turn>user\\n{content}<end_of_turn>\\n\"\n",
    "        elif role == \"model\" or role == \"assistant\": \n",
    "            formatted_text += f\"<start_of_turn>model\\n{content}<end_of_turn>\\n\"\n",
    "        else:\n",
    "            formatted_text += f\"<start_of_turn>{role}\\n{content}<end_of_turn>\\n\"\n",
    "    formatted_text += tokenizer.eos_token\n",
    "    row[\"text\"] = formatted_text\n",
    "    return row\n",
    "\n",
    "dataset = dataset.map(format_chat_template)\n",
    "\n",
    "# --- 3. LoRA Configuration ---\n",
    "peft_config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "# --- 4. Training Arguments (Original Hyperparameters) ---\n",
    "sft_config = SFTConfig(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=8,  # ‡∏Ñ‡∏á‡πÑ‡∏ß‡πâ‡∏ï‡∏≤‡∏°‡πÄ‡∏î‡∏¥‡∏°\n",
    "    gradient_accumulation_steps=4,  # ‡∏Ñ‡∏á‡πÑ‡∏ß‡πâ‡∏ï‡∏≤‡∏°‡πÄ‡∏î‡∏¥‡∏°\n",
    "    learning_rate=1e-4,             # ‡∏Ñ‡∏á‡πÑ‡∏ß‡πâ‡∏ï‡∏≤‡∏°‡πÄ‡∏î‡∏¥‡∏°\n",
    "    weight_decay=0.01,\n",
    "    bf16=True,\n",
    "    tf32=True,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    report_to=\"wandb\",\n",
    "    run_name=f\"{NEW_MODEL_NAME}-H100-run\",\n",
    "    max_grad_norm=1.0,\n",
    "    warmup_steps=100,\n",
    "    group_by_length=True,\n",
    "    push_to_hub=True,\n",
    "    hub_model_id=HUB_REPO_ID,\n",
    "    hub_strategy=\"every_save\",\n",
    "    max_length=4096,\n",
    "    packing=False,\n",
    "    dataset_text_field=\"text\"\n",
    ")\n",
    "\n",
    "# --- 5. Trainer ---\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=sft_config,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    processing_class=tokenizer \n",
    "    # ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏™‡πà formatting_func ‡∏ã‡πâ‡∏≥ ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÄ‡∏£‡∏≤ map ‡∏•‡∏á column 'text' ‡πÅ‡∏•‡πâ‡∏ß\n",
    ")\n",
    "\n",
    "# --- 6. Resume & Train ---\n",
    "print(\"üöÄ Starting training on H100 (SDPA Mode)...\")\n",
    "last_checkpoint = get_last_checkpoint(OUTPUT_DIR)\n",
    "if last_checkpoint:\n",
    "    trainer.train(resume_from_checkpoint=last_checkpoint)\n",
    "else:\n",
    "    trainer.train()\n",
    "\n",
    "# --- 7. Save & Push ---\n",
    "print(\"Pushing final model...\")\n",
    "trainer.save_model(NEW_MODEL_NAME)\n",
    "tokenizer.save_pretrained(NEW_MODEL_NAME)\n",
    "trainer.push_to_hub()\n",
    "# Tokenizer push ‡πÅ‡∏¢‡∏Å‡πÄ‡∏ú‡∏∑‡πà‡∏≠ trainer ‡πÑ‡∏°‡πà‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏ö\n",
    "tokenizer.push_to_hub(HUB_REPO_ID) \n",
    "print(\"‚úÖ Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33561c8-c67c-49a8-bcd8-b0b012660ce6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
